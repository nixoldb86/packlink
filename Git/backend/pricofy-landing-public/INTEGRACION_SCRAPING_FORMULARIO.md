# üîó Integraci√≥n de Scraping en Formulario de Solicitud

Este documento explica c√≥mo se ha integrado el sistema de scraping autom√°tico cuando un usuario env√≠a un formulario de evaluaci√≥n.

## üìã Resumen de Cambios

Se ha reemplazado la **evaluaci√≥n con ChatGPT** por un **proceso de scraping autom√°tico** que busca anuncios reales en Wallapop y Milanuncios, manteniendo solo el uso de ChatGPT para generar variantes de b√∫squeda laxa.

---

## üîÑ Flujo Completo

```
Usuario rellena formulario
        ‚Üì
Validar datos (email, campos obligatorios, fotos si vende)
        ‚Üì
Verificar l√≠mite (1 solicitud por email/d√≠a)
        ‚Üì
Subir fotos (Backblaze B2 o local)
        ‚Üì
Guardar solicitud en BD ‚Üí obtener solicitud_id
        ‚Üì
üï∑Ô∏è EJECUTAR SCRAPING AUTOM√ÅTICO
   ‚îú‚îÄ‚îÄ Mapear tipo_producto ‚Üí categor√≠a
   ‚îú‚îÄ‚îÄ Mapear estado ‚Üí condici√≥n_objetivo
   ‚îú‚îÄ‚îÄ Construir ubicaci√≥n (pa√≠s/ciudad)
   ‚îú‚îÄ‚îÄ Ejecutar scraping con runScraping()
   ‚îÇ   ‚îî‚îÄ‚îÄ ChatGPT solo para b√∫squeda laxa (si es necesario)
   ‚îî‚îÄ‚îÄ Guardar resultados en scraping_results
        ‚Üì
Enviar email de confirmaci√≥n al usuario
        ‚Üì
Responder al cliente con success
```

**Importante:** El scraping NO bloquea el flujo. Si falla, la solicitud ya est√° guardada y el usuario recibe confirmaci√≥n.

---

## üìù Archivos Modificados

### 1Ô∏è‚É£ `app/api/submit-request/route.ts`

**Cambios principales:**

#### Antes (l√≠neas 5-9):
```typescript
import { saveSolicitud, checkSolicitudToday, saveEvaluacion } from '@/lib/db'
import { sendEvaluationConfirmationEmail, type EvaluationData } from '@/lib/email'
import { generateEvaluation } from '@/lib/chatgpt'
```

#### Ahora:
```typescript
import { saveSolicitud, checkSolicitudToday, saveScrapingResults } from '@/lib/db'
import { sendEvaluationConfirmationEmail, type EvaluationData } from '@/lib/email'
import { runScraping } from '@/lib/scraper'
```

#### L√≥gica de Scraping (l√≠neas 139-210):

```typescript
// Ejecutar scraping autom√°tico (no bloquea si falla)
if (insertId) {
  try {
    console.log('üï∑Ô∏è Iniciando scraping autom√°tico para solicitud ID:', insertId)
    
    // Mapear tipo de producto a categor√≠a
    const categoriasMap: Record<string, string> = {
      'electronica': 'electronica',
      'electrodom√©sticos': 'electrodomesticos',
      'hogar y jard√≠n': 'hogar',
      'moda y accesorios': 'moda',
      'deportes y ocio': 'deporte',
      'motor': 'motor',
      'otros': 'general',
    }
    const categoria = categoriasMap[data.tipoProducto?.toLowerCase()] || 'general'
    
    // Mapear estado a condici√≥n objetivo
    const estadosMap: Record<string, string> = {
      'nuevo': 'nuevo',
      'como nuevo': 'como_nuevo',
      'muy buen estado': 'muy_buen_estado',
      'buen estado': 'buen_estado',
      'usado': 'usado',
      'aceptable': 'aceptable',
    }
    const condicion = estadosMap[data.estado?.toLowerCase()] || 'buen_estado'
    
    // Determinar idioma de b√∫squeda
    const acceptLanguage = request.headers.get('accept-language') || ''
    const idioma = acceptLanguage.includes('en') && !acceptLanguage.includes('es') ? 'en' : 'es'
    
    // Construir ubicaci√≥n (formato: "pa√≠s/ciudad")
    const ubicacion = `${data.pais}/${data.ciudad}`.toLowerCase()
    
    // Ejecutar scraping
    const scrapingResult = await runScraping({
      producto_text: data.modeloMarca,
      categoria: categoria,
      ubicacion: ubicacion,
      radio_km: 30, // Radio por defecto: 30 km
      condicion_objetivo: condicion as any,
      idioma_busqueda: idioma,
      min_paginas_por_plataforma: 100,
      min_resultados_por_plataforma: 250,
    })

    if (scrapingResult) {
      // Guardar resultados del scraping en la base de datos
      await saveScrapingResults(
        insertId,
        {
          producto_text: data.modeloMarca,
          categoria: categoria,
          ubicacion: ubicacion,
          radio_km: 30,
          condicion_objetivo: condicion,
        },
        scrapingResult
      )
      console.log('‚úÖ Scraping ejecutado y guardado correctamente')
      console.log(`   - Compradores: ${scrapingResult.jsonCompradores?.compradores?.length || 0} anuncios`)
      console.log(`   - Vendedores: ${scrapingResult.jsonVendedores?.vendedores?.length || 0} precios`)
    } else {
      console.warn('‚ö†Ô∏è No se pudo ejecutar el scraping')
    }
  } catch (scrapingError) {
    console.error('‚ùå Error en el proceso de scraping autom√°tico (no cr√≠tico):', scrapingError)
    // No bloqueamos el flujo - la solicitud ya est√° guardada
  }
}
```

---

### 2Ô∏è‚É£ `lib/db.ts`

**Funci√≥n nueva:** `saveScrapingResults()`

Reemplaza a `saveEvaluacion()` y guarda:

```typescript
export async function saveScrapingResults(
  solicitudId: number,
  searchParams: {
    producto_text: string
    categoria: string
    ubicacion: string
    radio_km: number
    condicion_objetivo: string
  },
  results: {
    tablaCompradores: any[]
    tablaVendedores: any[]
    jsonCompradores: any
    jsonVendedores: any
  }
): Promise<{ insertId: number }>
```

**Datos que guarda:**
- Par√°metros de b√∫squeda (producto, categor√≠a, ubicaci√≥n, radio, condici√≥n)
- Resultados completos en formato JSON
- Metadatos (total anuncios encontrados, filtrados, plataformas)

---

### 3Ô∏è‚É£ Nueva Tabla SQL: `scraping_results`

**Archivo:** `CREATE_TABLE_SCRAPING_RESULTS.sql`

**Estructura:**

```sql
CREATE TABLE IF NOT EXISTS scraping_results (
    id SERIAL PRIMARY KEY,
    solicitud_id INTEGER NOT NULL REFERENCES solicitudes(id) ON DELETE CASCADE,

    -- Par√°metros de b√∫squeda
    producto_text VARCHAR(255) NOT NULL,
    categoria VARCHAR(100) NOT NULL,
    ubicacion VARCHAR(200) NOT NULL,
    radio_km INTEGER NOT NULL,
    condicion_objetivo VARCHAR(50) NOT NULL,

    -- Resultados JSON
    json_compradores JSONB,
    json_vendedores JSONB,
    tabla_compradores JSONB,
    tabla_vendedores JSONB,

    -- Todas las URLs encontradas (sin filtrar)
    todas_urls_encontradas TEXT[],

    -- Metadatos
    total_anuncios_encontrados INTEGER DEFAULT 0,
    total_anuncios_filtrados INTEGER DEFAULT 0,
    plataformas_consultadas TEXT[],

    -- Timestamps
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**Nueva Columna:** `todas_urls_encontradas TEXT[]`
- Array con **todas** las URLs encontradas durante el scraping
- **Sin filtrar** (incluye URLs descartadas por relevancia, outliers, etc.)
- √ötil para auditor√≠a y debugging
- Puede ser consultada con `unnest(todas_urls_encontradas)` en PostgreSQL

**√çndices:**
- `idx_scraping_results_solicitud_id` ‚Üí Para JOIN con `solicitudes`
- `idx_scraping_results_created_at` ‚Üí Para ordenar por fecha
- `idx_scraping_results_producto_text` ‚Üí Para b√∫squedas por producto

---

## üéØ Mapeos Autom√°ticos

### Tipo de Producto ‚Üí Categor√≠a de Scraping

| Tipo Producto (formulario) | Categor√≠a (scraping) |
|----------------------------|----------------------|
| Electr√≥nica | `electronica` |
| Electrodom√©sticos | `electrodomesticos` |
| Hogar y jard√≠n | `hogar` |
| Moda y accesorios | `moda` |
| Deportes y ocio | `deporte` |
| Motor | `motor` |
| Otros | `general` |

### Estado ‚Üí Condici√≥n Objetivo

| Estado (formulario) | Condici√≥n (scraping) |
|--------------------|----------------------|
| Nuevo | `nuevo` |
| Como nuevo | `como_nuevo` |
| Muy buen estado | `muy_buen_estado` |
| Buen estado | `buen_estado` |
| Usado | `usado` |
| Aceptable | `aceptable` |

---

## üìä Par√°metros de Scraping

Los siguientes par√°metros se usan autom√°ticamente:

```typescript
{
  producto_text: data.modeloMarca,           // Ej: "iPhone 17 Pro 512GB"
  categoria: categoriaMapeada,               // Ej: "electronica"
  ubicacion: `${pais}/${ciudad}`,            // Ej: "espa√±a/madrid"
  radio_km: 30,                              // Radio fijo: 30 km
  condicion_objetivo: condicionMapeada,      // Ej: "buen_estado"
  idioma_busqueda: 'es' | 'en',              // Detectado autom√°ticamente
  min_paginas_por_plataforma: 100,           // P√°ginas por plataforma
  min_resultados_por_plataforma: 250,        // Anuncios m√≠nimos por plataforma
}
```

**Nota:** Puedes ajustar `radio_km`, `min_paginas_por_plataforma` y `min_resultados_por_plataforma` seg√∫n necesites.

---

## ü§ñ Uso de ChatGPT

ChatGPT **solo se usa** en un caso espec√≠fico:

### B√∫squeda Laxa (Lazy Search)

Si la primera b√∫squeda estricta **no encuentra suficientes resultados** (< 250 anuncios relevantes), el sistema:

1. Llama a `generateSearchVariants()` de ChatGPT
2. ChatGPT genera 5 variantes de b√∫squeda optimizadas
3. Se ejecuta una segunda b√∫squeda con esas variantes
4. **Fallback:** Si ChatGPT falla, usa `generarVariantesBusqueda()` (manual)

**Prompt usado por ChatGPT:**
```
Act√∫a como experto en b√∫squeda de anuncios de segunda mano. 
A partir de product_text="..." e idioma="...", 
genera exactamente 5 cadenas de b√∫squeda distintas, 
optimizadas para t√≠tulos/listados.

Reglas:
- Solo texto plano (palabras separadas por espacios)
- Incluye variantes de marca/modelo, abreviaturas y errores comunes
- Prioriza tokens de alto valor (marca, modelo, medida, color)
- Evita stopwords y relleno

Salida: solo 5 l√≠neas, cada l√≠nea una cadena; sin numeraci√≥n ni texto extra.
```

---

## üì¶ Resultados Guardados

Cada scraping guarda **dos tipos de datos**:

#### üîç URLs Encontradas (Todas)
- **Columna:** `todas_urls_encontradas TEXT[]`
- **Contenido:** Array con **todas** las URLs encontradas durante el scraping
- **Sin filtros:** Incluye URLs descartadas por relevancia, outliers, duplicados, etc.
- **Prop√≥sito:** Auditor√≠a, debugging, an√°lisis de cobertura

#### ‚úÖ URLs Finales (Filtradas)
- **Contenido:** Solo las URLs que pasan todos los filtros y aparecen en resultados
- **Con filtros:** Relevancia, outliers, duplicados, condici√≥n m√≠nima, etc.
- **Prop√≥sito:** Datos finales para mostrar al usuario

### Ejemplo de Diferencia

Para una b√∫squeda de "iPhone 17 Pro 512GB":

| Tipo | Cantidad | Ejemplo |
|------|----------|---------|
| **URLs Encontradas** | 150 URLs | wallapop.com/item/iphone-15-pro, wallapop.com/item/iphone-17-pro, ... |
| **URLs Procesadas** | 120 URLs | Despu√©s de normalizaci√≥n y filtros b√°sicos |
| **URLs Filtradas** | 28 URLs | Despu√©s de filtro de relevancia (70%) |
| **URLs Finales** | 25 URLs | Despu√©s de outliers y deduplicaci√≥n |

### JSON Compradores
```json
{
  "compradores": [
    {
      "plataforma": "wallapop",
      "precio_eur": 950.00,
      "estado_declarado": "como_nuevo",
      "ciudad_o_zona": "Madrid",
      "url_anuncio": "https://...",
      "url_listado": "https://...",
      "fecha_publicacion": "2025-01-15"
    },
    // ... m√°s anuncios
  ]
}
```

### JSON Vendedores
```json
{
  "vendedores": [
    {
      "tipo_precio": "minimo",
      "precio_eur": 800.00,
      "plataforma": "wallapop, milanuncios",
      "urls": ["https://...", "https://..."],
      "plataforma_sugerida": ["wallapop", "milanuncios"]
    },
    {
      "tipo_precio": "ideal",
      "precio_eur": 920.50,
      "plataforma": "wallapop, milanuncios",
      "urls": ["https://...", "https://..."],
      "plataforma_sugerida": ["wallapop", "milanuncios"]
    },
    {
      "tipo_precio": "rapido",
      "precio_eur": 828.45,
      "plataforma": "wallapop, milanuncios",
      "urls": ["https://...", "https://..."],
      "plataforma_sugerida": ["wallapop", "milanuncios"]
    }
  ],
  "descripcion_anuncio": "iPhone 17 Pro 512GB en muy buen estado..."
}
```

### Tablas
Similar pero en formato array de objetos, listas para mostrar en tablas.

---

## üîç Consultas SQL √ötiles

### Ver √∫ltimos scrapings ejecutados
```sql
SELECT 
    sr.id,
    sr.producto_text,
    sr.ubicacion,
    sr.total_anuncios_encontrados,
    sr.total_anuncios_filtrados,
    sr.plataformas_consultadas,
    sr.created_at,
    s.email,
    s.pais,
    s.ciudad
FROM scraping_results sr
JOIN solicitudes s ON sr.solicitud_id = s.id
ORDER BY sr.created_at DESC
LIMIT 10;
```

### Ver anuncios encontrados para un scraping espec√≠fico
```sql
SELECT 
    sr.json_compradores->'compradores' as anuncios_compradores,
    sr.json_vendedores->'vendedores' as precios_vendedor
FROM scraping_results sr
WHERE sr.id = 123;  -- Cambiar por el ID del scraping
```

### Estad√≠sticas de scraping
```sql
SELECT
    sr.categoria,
    COUNT(*) as total_scrapings,
    AVG(sr.total_anuncios_encontrados) as promedio_anuncios,
    AVG(sr.total_anuncios_filtrados) as promedio_filtrados,
    AVG(array_length(sr.todas_urls_encontradas, 1)) as promedio_urls_encontradas
FROM scraping_results sr
GROUP BY sr.categoria
ORDER BY total_scrapings DESC;
```

### Ver todas las URLs encontradas en un scraping
```sql
SELECT
    sr.id,
    sr.producto_text,
    array_length(sr.todas_urls_encontradas, 1) as total_urls,
    unnest(sr.todas_urls_encontradas) as url_encontrada
FROM scraping_results sr
WHERE sr.id = 123;  -- Cambiar por el ID del scraping
```

### Comparar URLs encontradas vs URLs finales
```sql
SELECT
    sr.id,
    sr.producto_text,
    array_length(sr.todas_urls_encontradas, 1) as urls_encontradas,
    sr.total_anuncios_encontrados as urls_procesadas,
    sr.total_anuncios_filtrados as urls_finales,
    jsonb_array_length(sr.json_compradores->'compradores') as compradores_finales
FROM scraping_results sr
WHERE sr.id = 123;
```

---

## ‚öôÔ∏è Configuraci√≥n Requerida

### 1. Crear la Tabla en Supabase

Ejecuta `CREATE_TABLE_SCRAPING_RESULTS.sql` en el SQL Editor de Supabase.

#### Si ya tienes la tabla sin la columna `todas_urls_encontradas`:

Ejecuta `ADD_TODAS_URLS_SCRAPING_RESULTS.sql` para a√±adir la nueva columna:

```sql
ALTER TABLE scraping_results
ADD COLUMN IF NOT EXISTS todas_urls_encontradas TEXT[];
```

### 2. Variables de Entorno

Aseg√∫rate de tener configuradas:

```bash
# Base de datos (Supabase)
POSTGRES_HOST=xxx.supabase.co
POSTGRES_PORT=6543
POSTGRES_USER=postgres.xxx
POSTGRES_PASSWORD=xxx
POSTGRES_DB=postgres

# ChatGPT (para b√∫squeda laxa)
OPENAI_API_KEY=sk-xxx
OPENAI_MODEL=gpt-4o-mini  # Opcional, por defecto usa gpt-4o-mini

# Puppeteer (para Wallapop scraping)
# No requiere configuraci√≥n adicional, se instala con npm install
```

### 3. Instalar Dependencias

Si a√∫n no lo has hecho:

```bash
npm install puppeteer cheerio undici openai
```

---

## üêõ Troubleshooting

### El scraping falla pero el formulario funciona

‚úÖ **Esto es correcto.** El scraping no bloquea el flujo. La solicitud se guarda y el usuario recibe confirmaci√≥n aunque el scraping falle.

### No se guardan resultados de scraping

1. Verifica que la tabla `scraping_results` existe:
   ```sql
   SELECT * FROM scraping_results LIMIT 1;
   ```

2. Revisa los logs del servidor:
   ```
   üï∑Ô∏è Iniciando scraping autom√°tico para solicitud ID: X
   ‚úÖ Scraping ejecutado y guardado correctamente
   ```

3. Si ves `‚ùå Error en el proceso de scraping autom√°tico`, revisa el mensaje de error espec√≠fico.

### ChatGPT no genera variantes

Si ves:
```
‚ö†Ô∏è [Processor] ChatGPT no disponible, usando generaci√≥n de variantes por defecto
```

Verifica:
1. `OPENAI_API_KEY` est√° configurada en `.env.local` y Vercel
2. Tienes cr√©ditos en tu cuenta de OpenAI
3. El modelo (`gpt-4o-mini` o el que uses) est√° disponible

**Nota:** El sistema funcionar√° con variantes manuales si ChatGPT falla.

### Puppeteer falla en Vercel

Puppeteer puede no funcionar en entornos serverless como Vercel. Considera:
1. Usar una funci√≥n separada en un servidor con Node.js completo
2. Usar un servicio de scraping como BrightData o ScraperAPI
3. Implementar el scraping como un job as√≠ncrono (cron, queue)

---

## üìö Documentaci√≥n Relacionada

| Archivo | Descripci√≥n |
|---------|-------------|
| `GUIA_SCRAPING.md` | Gu√≠a completa del sistema de scraping |
| `PORQUE_SE_DESCARTAN_ANUNCIOS.md` | Explicaci√≥n de filtros de relevancia |
| `MEJORAS_RELEVANCIA_IMPLEMENTADAS.md` | Sistema de relevancia y umbrales |
| `VARIANTES_CHATGPT.md` | Generaci√≥n de variantes con ChatGPT |
| `CREATE_TABLE_SCRAPING_RESULTS.sql` | Script SQL para crear la tabla |
| `lib/scraper/EJEMPLO_USO.ts` | Ejemplo de uso del scraping |

---

## ‚úÖ Ventajas de Esta Implementaci√≥n

1. **‚úÖ Datos Reales:** Los usuarios reciben precios reales del mercado (no estimaciones de IA)
2. **‚úÖ No Bloquea el Flujo:** El scraping es as√≠ncrono, el usuario no espera
3. **‚úÖ Robusto:** Fallback manual si ChatGPT falla
4. **‚úÖ Escalable:** F√°cil a√±adir m√°s plataformas de scraping
5. **‚úÖ Trazable:** Todos los resultados quedan guardados en BD
6. **‚úÖ Optimizado:** Uso m√≠nimo de ChatGPT (solo para b√∫squeda laxa)

---

## üöÄ Pr√≥ximos Pasos Sugeridos

1. **Implementar vista en `/admin`** para ver resultados de scraping
2. **A√±adir Milanuncios scraper** (actualmente solo Wallapop est√° completo)
3. **Enviar PDF con resultados** al email del usuario (usar puppeteer o jsPDF)
4. **Job as√≠ncrono** para scrapings largos (usando Bull, BullMQ, o similar)
5. **Cach√© de resultados** para productos populares (evitar scraping duplicado)
6. **Rate limiting** para evitar baneos de Wallapop/Milanuncios

---

## üí° Conclusi√≥n

El sistema ahora proporciona **datos reales del mercado** en lugar de estimaciones de IA, mientras mantiene un flujo de usuario r√°pido y robusto. ChatGPT se usa de forma estrat√©gica solo cuando es necesario (b√∫squeda laxa), minimizando costos y maximizando precisi√≥n.

